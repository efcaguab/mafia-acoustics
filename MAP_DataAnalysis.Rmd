---
title: "Mafia Acoustic Paper - Data Analysis"
author: "Fernando Cagua"
date: "1/26/2015"
output:
  html_document:
    css: style.css
    highlite: kate
    toc: yes
---

```{r, include=FALSE}
library (plyr)
library (dplyr)
library (magrittr)
library (ggplot2)
library (VTrack)
library (lubridate)
library (foreach); registerDoMC (cores = 30)
library (knitr)
```


Here we re-analyze the whale shark acoustic and visual data. Cleaner and nicer than before.

# Preprocessing

## Acoustic data
We first import the all files that contain the data we want to work with. That includes the csv containing the raw detections as exported by VUE, the receiver events file (from VUE), the array events file and the list of whale sharks tagged. And the list with sharks known to have lost their tag.

```{r}
# Read CSV detection file and process it with VTrack
MAFIA.DETECTIONS <- ReadInputData (
  read.csv ("../Raw Data/AllMafiaDetections_20141210.csv"),
  iHoursToAdd = 3) %>% tbl_df ()
MAFIA.DETECTIONS$DATETIME <- as.POSIXct (
  as.POSIXlt (MAFIA.DETECTIONS$DATETIME, tz="Africa/Dar_es_Salaam"))

RECEIVER.EVENTS <- read.csv ("../Raw Data/AllMafiaEvents_20141210.csv")
ARRAY.EVENTS <- read.csv ("../Raw Data/ArrayEvents_20141211.csv")
WS.TAGS <- read.csv ("../Raw Data/WSTags_20140909.csv")
```

### Correct time drift

We also correct for time drift using the events exported from VUE. It is necessary to check manually that all entries are correct, i.e. to check that all the time zones were correctly set up. We found four inconsistencies.

```{r}
# Read CSV events file 

names (RECEIVER.EVENTS) <- c ("DATETIME", "RECEIVERID", "DESC", "DATA", "UNITS")
RECEIVER.EVENTS$DATETIME <- as.POSIXct (RECEIVER.EVENTS$DATETIME, tz ="UTC")
RECEIVER.EVENTS$DATETIME <- as.POSIXct (
  as.POSIXlt (RECEIVER.EVENTS$DATETIME, tz="Africa/Dar_es_Salaam"))
PC.TIMES <- RECEIVER.EVENTS [RECEIVER.EVENTS$DESC == "PC Time", c (1, 2, 4)]
# We manually checked that all PC times are in the time-zone GMT+3 so we can go ahead and convert the PC times to POSIXct class
PC.TIMES$DATA <- as.POSIXct (
  substr (as.character (PC.TIMES$DATA), 1, 19), tz="Africa/Dar_es_Salaam")

# # Plots to check for time zone and computer time mistakes
# ggplot(PC.TIMES) + geom_line (aes (x = DATA, y = as.vector (DATA - DATETIME)/60)) + facet_grid (. ~ RECEIVERID , scale = "fixed")
# U <- PC.TIMES[PC.TIMES$RECEIVERID == "VR2W-104845", ]
# V <- data.frame (DATETIME = seq (min(U$DATETIME), max(U$DATETIME), "week"), DIFF = approx (as.numeric(U$DATA), as.vector (U$DATA - U$DATETIME), seq (min(U$DATETIME), max(U$DATETIME), "week")))
# ggplot

# Error for Data Upload on 2013-01-27 09:21:07 (receiver time) for VR2W-104847
PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-27 09:21:07")] <- PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-27 09:21:07")] + 3600*3
# Error for Data Upload on 2013-01-22 11:37:46 (receiver time) for VR2W-104848
PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-22 11:37:46")] <- PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-22 11:37:46")] + 3600*3
# Error for Data Upload on 2013-01-27 08:12:34 (receiver time) for VR2W-109044
PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-27 08:12:34")] <- PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-27 08:12:34")] + 3600*3
# Error for Data Upload on 2013-01-27 07:42:54 (receiver time) for VR2W-113484
PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-27 07:42:54")] <- PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-27 07:42:54")]  + 3600*3

# Correct time drift 
receiverIDs <- levels (MAFIA.DETECTIONS$RECEIVERID)
#pb <- txtProgressBar(max=length (receiverIDs), style = 3)
for (i in 1:length (receiverIDs)){
  #setTxtProgressBar (pb, i)
  receiver.PC.TIMES <- PC.TIMES[PC.TIMES$RECEIVERID == receiverIDs[i], ]
  drift <- approx (receiver.PC.TIMES$DATA, receiver.PC.TIMES$DATA - receiver.PC.TIMES$DATETIME, MAFIA.DETECTIONS[MAFIA.DETECTIONS$RECEIVERID == receiverIDs[i], ]$DATETIME)$y
  MAFIA.DETECTIONS[MAFIA.DETECTIONS$RECEIVERID == receiverIDs[i], ]$DATETIME <- MAFIA.DETECTIONS[MAFIA.DETECTIONS$RECEIVERID == receiverIDs[i], ]$DATETIME + drift
}
#close (pb)
```

### Assign detections to stations

In order to assign detections to stations we import a table containing a list of the times when a receiver was deployed or retrieved and where. Each detection is then assigned to a particular station (location) as opposed to a specific receiver. This procedure automatically takes out detections that occurred when receivers were outside of the water.

```{r}
# Read and organize events (retrievals/deployments) data
ARRAY.EVENTS <- ARRAY.EVENTS %>% tbl_df () %>%
  filter (EVENT == "DEP" | EVENT == "RET") %>%
  mutate (DATETIME = as.POSIXct (DATE, format = "%d/%m/%y %H:%M", tz = "Africa/Dar_es_Salaam"),
          STATIONNAME = as.factor (STATION),
          RECEIVERID = REC) %>%
  select (DATETIME, STATIONNAME, EVENT, RECEIVERID)

# Read stations file and assign detections to stations
STATIONS <- read.csv ("../Raw Data/Stations_20130205.csv")
# Assign station and location

for (i in 1:nrow (ARRAY.EVENTS)){  # For each event
  # If is a deployment change the station for the future
  if (ARRAY.EVENTS$EVENT[i] == "DEP"){  
    
    replace.index <- (as.character (ARRAY.EVENTS$RECEIVERID[i]) == as.character(MAFIA.DETECTIONS$RECEIVERID)) & (MAFIA.DETECTIONS$DATETIME >= ARRAY.EVENTS$DATETIME[i]) 
    # Include station
    MAFIA.DETECTIONS$STATIONNAME [replace.index] <- as.character (ARRAY.EVENTS$STATIONNAME[i])
  }
  # If is a retrieval delete data for the future
  else {  
    replace.index <- (as.character (ARRAY.EVENTS$RECEIVERID[i]) == as.character(MAFIA.DETECTIONS$RECEIVERID)) & 
      (MAFIA.DETECTIONS$DATETIME >= ARRAY.EVENTS$DATETIME[i]) 
    MAFIA.DETECTIONS$STATIONNAME[replace.index] <- NA
  }
}


#close (pb)
# Delete detections outside valid intervals
MAFIA.DETECTIONS <- MAFIA.DETECTIONS %>% filter (STATIONNAME != "Unknown", !is.na (STATIONNAME))

save (MAFIA.DETECTIONS, ARRAY.EVENTS, file ="../Processed Data/AllDetections.RData")
rm (i, replace.index)
```

### Filter detections

Here we filter out only whale shark tags (discarding range test, foreign and collisions). I use the list of tagged sharks for that. Simultaneously, to prevent the analysis of potentially un-natural behavior the first 48 hours of detections are removed. It also assigns a unique whale shark ID to each transmitter ID.

```{r}
# Read file with Whale Shark Tag lists
WS.TAGS$DATE <- as.POSIXct (WS.TAGS$DATE, format="%d/%m/%Y", tz = "Africa/Dar_es_Salaam")
WS.TAGS$NAME <- WS.TAGS$COMMENT <- WS.TAGS$SHARK <- NULL

# Select only whale shark detections 
DET.WS <- MAFIA.DETECTIONS[!is.na (match (MAFIA.DETECTIONS$TRANSMITTERID, WS.TAGS$TRANSMITTERID)), ]

# Remove detections before 48 hours after tagging date
for (i in 1: nrow(WS.TAGS)){
  next2.days <- WS.TAGS$DATE + 60 * 60 * 24 * 2  # Add two days
  replace.index <- as.character (DET.WS$TRANSMITTERID) == as.character (WS.TAGS$TRANSMITTERID[i])
  # Delete rows that are in the tagging day
  DET.WS <- subset (DET.WS, ! (replace.index & (DET.WS$DATETIME < next2.days[i]))) 
}

# Remove unused tags from the factor list
DET.WS$TRANSMITTERID <- factor (DET.WS$TRANSMITTERID)
rm (i, next2.days, replace.index)
```

### Foreign tags

Remove range test tags and whale shark tags. There was an “abbandoned” tag in fron of K11 for a long time. We’ll remove it. Also Because some of them might be colissions, we’ll discard any with less than two detections

```{r}
range.test.tags <- read.csv ("../Raw Data/RTTags_20130305.csv")
DET.RT <- filter (MAFIA.DETECTIONS, TRANSMITTERID %in% range.test.tags$Transmitter)

ddet.ws <- filter (MAFIA.DETECTIONS, TRANSMITTERID %in% WS.TAGS$TRANSMITTERID)
foreign.detections <- anti_join (MAFIA.DETECTIONS, ddet.ws) %>%
  anti_join (DET.RT) %>% 
  filter (TRANSMITTERID != "A69-1303-53872") %>%
  select (-(STATIONNAME)) %>%
  ddply ("TRANSMITTERID", function (x){
  if (nrow (x) > 1) {
    return (x)
  } else {
    return (NULL)
    }
})
foreign.detections$TRANSMITTERID <- factor (foreign.detections$TRANSMITTERID)
```

# Residency models

First we write the function that calculates the probabilities. It includes the initiall as well as the final date.

```{r}
# Function to calculate the vectors of presence absence
pres.abs.lag <- function (start.date, end.date, sightings, dates){
  # Create a data frame with the detections
  sight <- data.frame (id = sightings, date = dates) %>%
    filter (date >= start.date, date <= end.date) %>%
    mutate (id = factor (id)) %>% 
    arrange (date)
  
  # For each shark we'll start with the first detection only
  individuals <- levels (sight$id)
  # Cycle trough each shark
  presence.absence <- foreach (i=1:length (individuals),
                               .combine = rbind) %dopar% {
    # Find dates in which the shark was present
    dates.present <- sight$date[sight$id == individuals[i]] %>%
      as.numeric ()
    # Establish all dates in which it was tagged (only dates in which there was monitoring)
    dates.tagged <- unique(sight$date)[unique(sight$date) > 
                                         sight$date[match (individuals[i], sight$id)]]
    # Find all possible combinations of dates in which it was tagged
    dates.comb <- as.data.frame (t (combn (dates.tagged, 2))) %>%
      tbl_df()
    names (dates.comb) <- c ("date.1", "date.2")
    dates.comb <- mutate (dates.comb, lag = date.2 - date.1, # Find the lag between given dates
                          # Establish if it was present for in that lag
                          present = (date.1 %in% dates.present) &
                            (date.2 %in% dates.present), 
                          date.1 = as.Date(date.1, origin = "1970-01-01"), 
                          date.2 = as.Date(date.2, origin = "1970-01-01"), 
                          id = individuals[i])
    return (dates.comb)
  }
  return (presence.absence)
}
```

## Acoustic data

Now we aggregate the detecton data into weeks. We also note that Emmental TZ-003 was tagged twice, but lost it’s first tag leaving no detections. The data frame aco.week now contains the information we need: ecocean number, sex, bacth and the date it was tagged.

```{r}
det.ws <- tbl_df (DET.WS)  # Convert to tbl data frame
ws.tags <- tbl_df (WS.TAGS)
names (det.ws) <- names (det.ws) %>% tolower ()  # change names to lower case 
names (ws.tags) <- names (ws.tags) %>% tolower ()
names (ws.tags)[2] <- "date.tag"

# Using only sharks present in the list of tagged sharks merge data frames
det.ws <- inner_join (det.ws, select (ws.tags, -(size), -(number)))

# Clump in a weekly basis
det.ws <- mutate (det.ws, date.week = cut (datetime, '2 week') %>% as.Date ())
aco.week <- ddply (det.ws, "date.week", function (det){
  sharks <- !duplicated (det$ecocean)
  per.week <- data.frame (ecocean = det$ecocean[sharks], 
                          sex = det$sex[sharks],
                          batch = det$batch[sharks], 
                          date.tag = det$date.tag[sharks])
}) %>% tbl_df ()
```

For this paper we decided we are only going to work with the data generated by the first 30 tagged sharks. We filter out all other sharks and calculate the response variables

```{r, message=FALSE, warning=FALSE}
# Filter only the first sharks
aco.week <- aco.week %>% filter (batch == "2012-1" | batch == "2012-2")

# Calculate probabilities
PADet <- pres.abs.lag (start.date = min (aco.week$date.week), 
                       end.date = max (aco.week$date.week),
                       sightings = aco.week$ecocean, 
                       dates = aco.week$date.week)
# Change "id" column to "ecocean"
names (PADet)[5] <- "ecocean"

# Put shark info back into the data-frame
PADet <- left_join (PADet, ws.tags %>% select (ecocean, sex, size, batch))
```

Before we continue we have to include introduce a variable of effort. In the case of the acoustic model it is the average number of working receivers per week.

```{r, message=FALSE}
# Arrange by date
array.events <- ARRAY.EVENTS %>% tbl_df () %>%
  arrange (DATETIME) 
# Change the ugly names in uppercase to lowercase
names (array.events) <- names (array.events) %>% tolower ()
array.events %<>% mutate_each (funs (as.character), -(datetime))

# Change the format of the events by date to a format by station
times.in <- ddply (array.events, "stationname", function (x){
  # Determine when a receiver was deployed in the station
  deployed <- x %>% filter (event == "DEP", lead (event) == "RET")
  # Determine when a receiver was retrieved from the station
  retrieved <- x %>% filter (event == "RET", lag (event) == "DEP")
  if (nrow (deployed) > 0) {
    y <- data.frame (station = first (x$stationname), 
                     date.in = deployed$datetime, 
                     date.out = retrieved$datetime,
                     rec.in = deployed$receiverid,
                     rec.out = retrieved$receiverid)
    return (y)
  } else return (NULL)
}) %>%  
  # Generate weekly approximations
  mutate (w.date.in = cut (date.in, 'week'),
                      w.date.out = cut (date.out, '2 week')) %>% tbl_df()

# For each re-detection date calculate the number of working stations
pres <- ddply (PADet, "date.2", function (x, times.in){
  # Filter out stations active at that date
    stations.listening <- filter (times.in, x$date.2[1] >= as.Date (w.date.in), x$date.2[1] <= as.Date (w.date.out)) %>% 
      select (station) %>%
      unique()
    # Create data frame with the list and the number of stations
  y <- data.frame (configuration = do.call (paste, as.list(stations.listening$station)), 
                   nStations = nrow (stations.listening))
    return (y)
  }, times.in = times.in) %>% tbl_df ()

# Merge with the Probability of acoustic detection data frame
PADet <- inner_join (PADet, pres %>% select (date.2, nStations)) %>%
  # Simplify factors by making them integers
  mutate (week.1 = week (date.1), 
          week.2 = week (date.2), 
          date.random = (as.numeric (date.1) - as.numeric (min (date.1))) / 7, 
          date.id = paste (date.1, ecocean))
```

### Remove lost tag data

Shed tags can make us believe the shark is absent, whereas in reality the shark might be still in the area but not be detected by the array.
We have visual confirmation on the sheding for some sharks, for those we look to the date in which they were last detected and ignore the future data and ignore all future presence/absence data. 
We do that to prevent false-absense to bias down the residency estimates. 
For the cases in which there is no visual confirmation of the tag shed, we use a conservative approach and assume the shark left the study area. 

```{r, message=FALSE}
# Read file with tag lost dates
tag.lost <- read.csv ("../Raw Data/shed_tag.csv") %>% tbl_df () %>%
  mutate (ecocean = as.character (ecocean), 
          transmitterid = as.character (transmitterid), 
          date.conf = as.POSIXct (date.conf, 
                                  format = "%Y-%m-%d", 
                                  tz = "Africa/Dar_es_Salaam") %>% as.Date (), 
          date.min = as.POSIXct (date.min, 
                                 format = "%Y-%m-%d", 
                                 tz = "Africa/Dar_es_Salaam") %>% as.Date ())

# Delete data corresponding to those sharks in the PADet table
PADet %<>% left_join (tag.lost %>% select (ecocean, date.min)) %>% {
  A <- filter (., date.1 <= date.min)
  B <- filter (., date.min %>% is.na)
  rbind (A, B)
}
```

### Fit models

To fit the models we use the library `gamm4` as opposed as the traditional `gamm` because although slower and less stable it offers better results for binary data.

#### Random component

```{r, eval = FALSE}
library (gamm4)

# Choose the random structure
ma.r.01 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + s (lag) + nStations + sex + size, family = "binomial", data = .)
ma.r.02 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + s (lag) + nStations + sex + size, family = "binomial", data = ., random= ~(1|ecocean))
ma.r.03 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + s (lag) + nStations + sex + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))

# Join instruction together so that they can be evaluated in parallel
ma.r <- c (ma.r.01, ma.r.02, ma.r.03)
# Evaluate the models
models.acoustic.random <- foreach (i = 1:length(ma.r)) %dopar% ma.r[[i]](PADet)
# Save them so that we don't have to wait for so long each time...
saveRDS (models.acoustic.random, "../Processed Data/models_acoustic_random.rds")
```

```{r}
# We convert the results into a data_frame for easy manipulation
models.acoustic.random <- 
  readRDS ("../Processed Data/models_acoustic_random.rds") %>% {
  data_frame (gam = lapply (., function (x) x$gam),
              mer = lapply (., function (x) x$mer))
} %>%
  mutate (AIC = mer %>% lapply (function (x) AIC (x)) %>% unlist,
          DAIC = AIC - min (AIC), 
          formula = mer %>% lapply (function (x) formula (x)) %>% as.character) %>%
  arrange (AIC)

models.acoustic.random %>%
  select (formula, AIC, DAIC) %>% 
  kable ()
```

The nest model by AIC is the one that includes both random factors (date nested into shark). 

#### Fixed component

```{r, eval = FALSE}
# We define the models
ma.f.01 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + s (lag) + nStations + sex + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.02 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + s (lag) + nStations + sex, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.03 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + s (lag) + nStations + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.04 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + s (lag) + sex + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.05 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + nStations + sex + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.06 <- . %>% gamm4 (present ~ s (lag) + nStations + sex + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.07 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + s (lag) + nStations, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.08 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + s (lag) + sex, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.09 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + nStations + sex, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.10 <- . %>% gamm4 (present ~ + s (lag) + nStations + sex, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.11 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + s (lag)  + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.12 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + nStations + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.13 <- . %>% gamm4 (present ~ s (lag) + nStations + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.14 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + sex + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.15 <- . %>% gamm4 (present ~ s (lag) + sex + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.16 <- . %>% gamm4 (present ~ nStations + sex + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.17 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + s (lag), family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.18 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + nStations, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.19 <- . %>% gamm4 (present ~ s (lag) + nStations, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.20 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + sex, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.21 <- . %>% gamm4 (present ~ s (lag) + sex, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.22 <- . %>% gamm4 (present ~ nStations + sex, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.23 <- . %>% gamm4 (present ~ s (week.2, bs = "cc") + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.24 <- . %>% gamm4 (present ~ s (lag) + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.25 <- . %>% gamm4 (present ~ sex + size, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.26 <- . %>% gamm4 (present ~ s (week.2, bs = "cc"), family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.27 <- . %>% gamm4 (present ~ s (lag), family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.28 <- . %>% gamm4 (present ~ nStations, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.29 <- . %>% gamm4 (present ~ sex, family = "binomial", data = ., random= ~(1|ecocean/date.random))
ma.f.30 <- . %>% gamm4 (present ~ size, family = "binomial", data = ., random= ~(1|ecocean/date.random))

# We join the instruction together
ma.f <- c (ma.f.01, ma.f.02, ma.f.03, ma.f.04, ma.f.05, ma.f.06, ma.f.07, ma.f.08, ma.f.09, ma.f.10, ma.f.11, ma.f.12, ma.f.13, ma.f.14, ma.f.15, ma.f.16, ma.f.17, ma.f.18, ma.f.19, ma.f.20, ma.f.21, ma.f.22, ma.f.23, ma.f.24, ma.f.25, ma.f.26, ma.f.27, ma.f.28, ma.f.29, ma.f.30)

# We fit the models and save the results
models.acoustic.fixed <- foreach (i = 1:length(ma.f)) %dopar% ma.f[[i]](PADet)
saveRDS (models.acoustic.fixed, file = "../Processed Data/models_acoustic_fixed.rds")
```

```{r}
models.acoustic.fixed <- 
  readRDS ("../Processed Data/models_acoustic_fixed.rds") %>% {
  data_frame (gam = lapply (., function (x) x$gam),
            mer = lapply (., function (x) x$mer))
} 

models.acoustic.fixed %<>% 
  mutate (AIC = mer %>% lapply (function (x) AIC(x)) %>% unlist,
          DAIC = AIC - min (AIC),
          formula = gam %>% lapply (function (x) formula(x)) %>% as.character) %>%
  arrange (AIC)

models.acoustic.fixed %>% 
  select (formula, AIC, DAIC) %>%
  kable ()

models.acoustic.fixed$gam[1][[1]] %>% plot (select = 1)
models.acoustic.fixed$gam[1][[1]] %>% plot (select = 2)
```

The strongest support is for the model with week, lag and effort, some degree of support also exists for the model that takes sex into account and the one that doesn't includes the number of stations. 
Remarkably week and lag are the only important predictors.

## Visual data

## Integration and prediction

# Depth and distance models

## Prepare data

We'll start by adding distance and depth information to the detection data frame

```{r}
station.distances <- read.csv ("../Raw Data/Stations_distance_20140923.csv")
det.ws.depth <- left_join (det.ws, station.distances) %>%
  mutate (depth = (sensor1 - 2)*3, distance = as.character (distance) %>% as.numeric ())
```

Now we determine if the detections are on the night or day based civil dawn and dusk

```{r}
# Mafia location
m.coord <- matrix (39.6, -7.9, nrow = 1, ncol = 2)
# Deposition
dep <- 6

library (maptools)

det.ws.depth <- mutate (det.ws.depth, date = trunc (datetime, units = "days") %>% as.POSIXct ()) %>%
  ddply ("date", function (x, m.coord, dep){
  c.dawn <- crepuscule (m.coord, first (x$date), POSIXct.out = TRUE, direction = "dawn", solarDep = dep)$time
  c.dusk <- crepuscule (m.coord, first (x$date), POSIXct.out = TRUE, direction = "dusk", solarDep = dep)$time
  for (i in 1:nrow (x)){
    if (x$datetime[i] < c.dawn | x$datetime[i] > c.dusk) {
    x$day.night.c[i] <- "NIGHT" 
  } else {
    x$day.night.c[i] <- "DAY"
  }
  }
  return (x)
}, m.coord = m.coord, dep = dep, .parallel = TRUE) %>% tbl_df()
```

Establish receiver configuration in case it's needed as a random factor

```{r}
pres.det <- ddply (det.ws.depth, "date", function (x, times.in){
  # Generate weekly approximations
    stations.listening <- filter (times.in, x$date[1] >= date.in, x$date[1] <= date.out) %>% 
      select (station) %>%
      unique()
  y <- data.frame (configuration = do.call (paste, as.list(stations.listening$station)), 
                   nStations = nrow (stations.listening))
    return (y)
  }, times.in = times.in) %>% tbl_df ()

det.ws.depth <- inner_join (det.ws.depth, pres.det)
```

Now we include shark size as well

```{r}
det.ws.depth <- inner_join (select (det.ws.depth, -(sex)), select (ws.tags, ecocean, size, sex)) %>%
  mutate (week = week (datetime)) %>%
  arrange (datetime)
```

Because of the large size of the data the model fitting will take forever.
Fuck that, let's clump it all in two-week bins, just as the previous analysis.

```{r}
det.ws.depth.2week <- det.ws.depth %>%
  mutate (date.week = datetime %>% cut ("2 week")) %>%
  group_by (date.week, ecocean, day.night.c) %>%
  summarise (size = first (size),
             sex = first (sex),
             week = mean (week, na.rm = T),
             depth = mean (depth, na.rm = T),
             distance = mean (distance, na.rm = T))
```

## Fit depth

First we start with the random component, 
```{r, message = FALSE}
library (mgcv)

# Basic model
md.r.01 <- . %>% gamm (depth ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., gamma = 1.4, method = "REML")
# With shark as a random factor
md.r.02 <- . %>% gamm (depth ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), gamma = 1.4, method = "REML")
# Ecocean + temporal autocorrelation
md.r.03 <- . %>% gamm (depth ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corAR1 (form = ~1|ecocean), gamma = 1.4, method = "REML")
md.r.04 <- . %>% gamm (depth ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 1, q = 1), gamma = 1.4, method = "REML")
md.r.05 <- . %>% gamm (depth ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 1), gamma = 1.4, method = "REML")
md.r.06 <- . %>% gamm (depth ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 1, q = 2), gamma = 1.4, method = "REML")
md.r.07 <- . %>% gamm (depth ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "REML")

md.r <- c (md.r.01, md.r.02, md.r.03, md.r.04, md.r.05, md.r.06, md.r.07)
models.depth.random <- foreach (i = 1:length(md.r)) %dopar% md.r[[i]](det.ws.depth.2week) %>% {
  data_frame (gam = lapply (., function (x) x$gam), 
              lme = lapply (., function (x) x$lme))
} %>% 
  mutate (AIC = lme %>% lapply (function (x) AIC(x)) %>% unlist,
          DAIC = AIC - min(AIC)) %>% 
  arrange (AIC)
  
```

The best random structure is the one that considers both the shark as a random factor and the correlation structure of second order for the moving average and the autocorrelation.

```{r}
md.f.01 <- . %>% gamm (depth ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
md.f.02 <- . %>% gamm (depth ~ s (week, bs = 'cr')  + size + sex, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
md.f.03 <- . %>% gamm (depth ~ s (week, bs = 'cr')  + size + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
md.f.04 <- . %>% gamm (depth ~ s (week, bs = 'cr') + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
md.f.05 <- . %>% gamm (depth ~ size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
md.f.06 <- . %>% gamm (depth ~ s (week, bs = 'cr')  + size , data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
md.f.07 <- . %>% gamm (depth ~ s (week, bs = 'cr') + sex, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
md.f.08 <- . %>% gamm (depth ~ size + sex, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
md.f.09 <- . %>% gamm (depth ~ s (week, bs = 'cr') + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
md.f.10 <- . %>% gamm (depth ~ size + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
md.f.11 <- . %>% gamm (depth ~ sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
md.f.12 <- . %>% gamm (depth ~ s (week, bs = 'cr'), data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
md.f.13 <- . %>% gamm (depth ~ size, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
md.f.14 <- . %>% gamm (depth ~ sex, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
md.f.15 <- . %>% gamm (depth ~ day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")

md.f <- c (md.f.01, md.f.02, md.f.03, md.f.04, md.f.05, md.f.06, md.f.07, md.f.08, md.f.09, md.f.10, md.f.11, md.f.12, md.f.13, md.f.14, md.f.15)

models.depth.fixed <- foreach (i = 1:length(md.f)) %dopar% md.f[[i]](det.ws.depth.2week) %>% {
  data_frame (gam = lapply (., function (x) x$gam), 
              lme = lapply (., function (x) x$lme))
} %>% 
  mutate (AIC = lme %>% lapply (function (x) AIC(x)) %>% unlist,
          DAIC = AIC - min(AIC), 
          formula = gam %>% lapply (function (x) formula(x)) %>% as.character) %>% 
  arrange (AIC)

models.depth.fixed %>% select (formula, AIC, DAIC) %>%
  kable ()
```

```{r}
models.depth.fixed$gam[1][[1]] %>% plot
models.depth.fixed$gam[1][[1]] %>% summary

```

The best model is the one that includes the time of the year and the time of the day. 

## Fit distance

First we start with the random component, 

```{r, message = FALSE}
library (mgcv)

# Basic model
ml.r.01 <- . %>% gamm (distance ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., gamma = 1.4, method = "REML")
# With shark as a random factor
ml.r.02 <- . %>% gamm (distance ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), gamma = 1.4, method = "REML")
# Ecocean + temporal autocorrelation
ml.r.03 <- . %>% gamm (distance ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corAR1 (form = ~1|ecocean), gamma = 1.4, method = "REML")
ml.r.04 <- . %>% gamm (distance ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 1, q = 1), gamma = 1.4, method = "REML")
ml.r.05 <- . %>% gamm (distance ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 1), gamma = 1.4, method = "REML")
ml.r.06 <- . %>% gamm (distance ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 1, q = 2), gamma = 1.4, method = "REML")
ml.r.07 <- . %>% gamm (distance ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "REML")

ml.r <- c (ml.r.01, ml.r.02, ml.r.03, ml.r.04, ml.r.05, ml.r.06, ml.r.07)
models.distance.random <- foreach (i = 1:length(ml.r)) %dopar% ml.r[[i]](det.ws.depth.2week) %>% {
  data_frame (gam = lapply (., function (x) x$gam), 
              lme = lapply (., function (x) x$lme))
} %>% 
  mutate (AIC = lme %>% lapply (function (x) AIC(x)) %>% unlist,
          DAIC = AIC - min(AIC)) %>% 
  arrange (AIC)
  
```

The best random structure is the one that considers both the shark as a random factor and the correlation structure of second order for the moving average and the autocorrelation.

Now we move to the fixed component

```{r}
ml.f.01 <- . %>% gamm (distance ~ s (week, bs = 'cr')  + size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
ml.f.02 <- . %>% gamm (distance ~ s (week, bs = 'cr')  + size + sex, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
ml.f.03 <- . %>% gamm (distance ~ s (week, bs = 'cr')  + size + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
ml.f.04 <- . %>% gamm (distance ~ s (week, bs = 'cr') + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
ml.f.05 <- . %>% gamm (distance ~ size + sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
ml.f.06 <- . %>% gamm (distance ~ s (week, bs = 'cr')  + size , data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
ml.f.07 <- . %>% gamm (distance ~ s (week, bs = 'cr') + sex, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
ml.f.08 <- . %>% gamm (distance ~ size + sex, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
ml.f.09 <- . %>% gamm (distance ~ s (week, bs = 'cr') + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
ml.f.10 <- . %>% gamm (distance ~ size + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
ml.f.11 <- . %>% gamm (distance ~ sex + day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
ml.f.12 <- . %>% gamm (distance ~ s (week, bs = 'cr'), data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
ml.f.13 <- . %>% gamm (distance ~ size, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
ml.f.14 <- . %>% gamm (distance ~ sex, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")
ml.f.15 <- . %>% gamm (distance ~ day.night.c, data = ., random = list (ecocean = ~ 1), correlation = corARMA (form = ~1|ecocean, p = 2, q = 2), gamma = 1.4, method = "ML")

ml.f <- c (ml.f.01, ml.f.02, ml.f.03, ml.f.04, ml.f.05, ml.f.06, ml.f.07, ml.f.08, ml.f.09, ml.f.10, ml.f.11, ml.f.12, ml.f.13, ml.f.14, ml.f.15)

models.distance.fixed <- foreach (i = 1:length(ml.f)) %dopar% ml.f[[i]](det.ws.depth.2week) %>% {
  data_frame (gam = lapply (., function (x) x$gam), 
              lme = lapply (., function (x) x$lme))
} %>% 
  mutate (AIC = lme %>% lapply (function (x) AIC(x)) %>% unlist,
          DAIC = AIC - min(AIC), 
          formula = gam %>% lapply (function (x) formula(x)) %>% as.character) %>% 
  arrange (AIC)

models.distance.fixed %>% select (formula, AIC, DAIC) %>%
  kable ()
```

The strongest support is for the model that includes size, sex and time of the day, but it's closely followed by the one that only includes size and sex. 

```{r}
models.distance.fixed$gam[1][[1]] %>% summary 
models.distance.fixed$gam[2][[1]] %>% summary 
models.distance.fixed$gam[2][[1]] %>% plot
```

Since the effect of time of the day is not significant in the first model, we'll go for the second one as the primary model for predictions. 