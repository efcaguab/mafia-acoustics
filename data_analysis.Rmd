---
title: "Mafia Acoustics Paper - Data Analysis"
author: "Fernando Cagua"
date: "9 September 2014"
output: html_document
---

```{r libraries and setup, include=FALSE, cache=FALSE}
# Turn on cache globally for faster report rendering
knitr::opts_chunk$set(cache=TRUE)

library (VTrack)  ## For organizing acoustic data (quite useless)
library (lubridate)  # Functions for handling date-time objects
library (mgcv) # Functions for GAMM
library (maptools)  # Functions for 
library (ggplot2)
library (doMC)
library (plyr)
library (dplyr)
library (reshape2)
registerDoMC (cores = 16)

```

# Preprocessing

## Acoustic data
We first import the all files that contain the data we want to work with. That includes the csv containing the raw detections as exported by VUE, the receiver events file (from VUE), the array events file and the list of whale sharks tagged. And the list with sharks known to have lost their tag.  

```{r read}
# Read CSV detection file and process it with VTrack
MAFIA.DETECTIONS <- ReadInputData (
  read.csv ("../Raw Data/AllMafiaDetections20140715.csv"),
  iHoursToAdd = 3)
MAFIA.DETECTIONS$DATETIME <- as.POSIXct (
  as.POSIXlt (MAFIA.DETECTIONS$DATETIME, tz="Africa/Dar_es_Salaam"))

RECEIVER.EVENTS <- read.csv ("../Raw Data/AllMafiaEvents20140715.csv")
ARRAY.EVENTS <- read.csv ("../Raw Data/ArrayEvents_20140808.csv")
WS.TAGS <- read.csv ("../Raw Data/WSTags_20140909.csv")

TAG.LOST <- data.frame (id = c ("X","TZ-030", "TZ-068", "TZ-040", "TZ-032"), 
                        date = as.Date (c ("2012-10-26","2013-10-14", "2013-12-21", "2014-01-07", "2014-01-09")))

```

### Correct time drift

We also correct for time drift using the events exported from VUE. It is necessary to check manually that all entries are correct, i.e. to check that all the time zones were correctly set up. We found four inconsistencies. 

```{r time drift}
# Read CSV events file 

names (RECEIVER.EVENTS) <- c ("DATETIME", "RECEIVERID", "DESC", "DATA", "UNITS")
RECEIVER.EVENTS$DATETIME <- as.POSIXct (RECEIVER.EVENTS$DATETIME, tz ="UTC")
RECEIVER.EVENTS$DATETIME <- as.POSIXct (
  as.POSIXlt (RECEIVER.EVENTS$DATETIME, tz="Africa/Dar_es_Salaam"))
PC.TIMES <- RECEIVER.EVENTS [RECEIVER.EVENTS$DESC == "PC Time", c (1, 2, 4)]
# We manually checked that all PC times are in the time-zone GMT+3 so we can go ahead and convert the PC times to POSIXct class
PC.TIMES$DATA <- as.POSIXct (
  substr (as.character (PC.TIMES$DATA), 1, 19), tz="Africa/Dar_es_Salaam")

# # Plots to check for time zone and computer time mistakes
# ggplot(PC.TIMES) + geom_line (aes (x = DATA, y = as.vector (DATA - DATETIME)/60)) + facet_grid (. ~ RECEIVERID , scale = "fixed")
# U <- PC.TIMES[PC.TIMES$RECEIVERID == "VR2W-104845", ]
# V <- data.frame (DATETIME = seq (min(U$DATETIME), max(U$DATETIME), "week"), DIFF = approx (as.numeric(U$DATA), as.vector (U$DATA - U$DATETIME), seq (min(U$DATETIME), max(U$DATETIME), "week")))
# ggplot

# Error for Data Upload on 2013-01-27 09:21:07 (receiver time) for VR2W-104847
PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-27 09:21:07")] <- PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-27 09:21:07")] + 3600*3
# Error for Data Upload on 2013-01-22 11:37:46 (receiver time) for VR2W-104848
PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-22 11:37:46")] <- PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-22 11:37:46")] + 3600*3
# Error for Data Upload on 2013-01-27 08:12:34 (receiver time) for VR2W-109044
PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-27 08:12:34")] <- PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-27 08:12:34")] + 3600*3
# Error for Data Upload on 2013-01-27 07:42:54 (receiver time) for VR2W-113484
PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-27 07:42:54")] <- PC.TIMES$DATA[PC.TIMES$DATETIME == as.POSIXct ("2013-01-27 07:42:54")]  + 3600*3

# Correct time drift 
receiverIDs <- levels (MAFIA.DETECTIONS$RECEIVERID)
#pb <- txtProgressBar(max=length (receiverIDs), style = 3)
for (i in 1:length (receiverIDs)){
  #setTxtProgressBar (pb, i)
  receiver.PC.TIMES <- PC.TIMES[PC.TIMES$RECEIVERID == receiverIDs[i], ]
  drift <- approx (receiver.PC.TIMES$DATA, receiver.PC.TIMES$DATA - receiver.PC.TIMES$DATETIME, MAFIA.DETECTIONS[MAFIA.DETECTIONS$RECEIVERID == receiverIDs[i], ]$DATETIME)$y
  MAFIA.DETECTIONS[MAFIA.DETECTIONS$RECEIVERID == receiverIDs[i], ]$DATETIME <- MAFIA.DETECTIONS[MAFIA.DETECTIONS$RECEIVERID == receiverIDs[i], ]$DATETIME + drift
}
#close (pb)
```

### Assign detections to stations

In order to assign detections to stations we import a table containing a list of the times when a receiver was deployed or retrieved and where. Each detection is then assigned to a particular station (location) as opposed to a specific receiver. This procedure automatically takes out detections that occurred when receivers were outside of the water.

```{r det to stat}
# Read and organize events (retrievals/deployments) data
ARRAY.EVENTS <- ARRAY.EVENTS[(ARRAY.EVENTS$EVENT == "DEP") | (ARRAY.EVENTS$EVENT == "RET"), ]
R.EVE <- data.frame(DATETIME = as.POSIXct (ARRAY.EVENTS$DATE, tz="Africa/Dar_es_Salaam")) 
R.EVE$STATIONNAME <- factor(ARRAY.EVENTS$STATION)
R.EVE$EVENT <- ARRAY.EVENTS$EVENT
R.EVE$RECEIVERID <- ARRAY.EVENTS$REC
ARRAY.EVENTS <- R.EVE

# Read stations file and assign detections to stations
STATIONS <- read.csv ("../Raw Data/Stations_20130205.csv")
# Assign station and location
#pb <- txtProgressBar(max=length (ARRAY.EVENTS [,1]), style = 3)
for (i in 1:length (ARRAY.EVENTS [,1])){  # For each event
  # If is a deployment change the station for the future
  if (ARRAY.EVENTS$EVENT[i] == "DEP"){  
    # message ("    Analyzing deployment of ", ARRAY.EVENTS$RECEIVERID[i], " on ", 
    #         floor_date (ARRAY.EVENTS$DATETIME[i], "day"), " at   Station ", ARRAY.EVENTS$STATIONNAME[i])
    replace.index <- (as.character (ARRAY.EVENTS$RECEIVERID[i]) == as.character(MAFIA.DETECTIONS$RECEIVERID)) & (MAFIA.DETECTIONS$DATETIME >= ARRAY.EVENTS$DATETIME[i]) 
    # Include station
    MAFIA.DETECTIONS$STATIONNAME [replace.index] <- as.character (ARRAY.EVENTS$STATIONNAME[i])
  }
  # If is a retrieval delete data for the future
  else {  
    # message ("    Analyzing retrieval  of ", ARRAY.EVENTS$RECEIVERID[i], " on ", 
    #        floor_date (ARRAY.EVENTS$DATETIME[i], "day"), " from Station ", ARRAY.EVENTS$STA[i])
    replace.index <- (as.character (ARRAY.EVENTS$RECEIVERID[i]) == as.character(MAFIA.DETECTIONS$RECEIVERID)) & 
      (MAFIA.DETECTIONS$DATETIME >= ARRAY.EVENTS$DATETIME[i]) 
    MAFIA.DETECTIONS$STATIONNAME[replace.index] <- NA
  }
  #setTxtProgressBar (pb, i)
}
#close (pb)
# Delete detections outside valid intervals
MAFIA.DETECTIONS <- MAFIA.DETECTIONS[MAFIA.DETECTIONS$STATIONNAME != 'Unknown' & !is.na (MAFIA.DETECTIONS$STATIONNAME), ]

save (MAFIA.DETECTIONS, ARRAY.EVENTS, file ="../Processed Data/AllDetections.RData")
rm (R.EVE, i, replace.index)
```

### Filter detections

Here we filter out only whale shark tags (discarding range test, foreign and collisions). I use the list of tagged sharks for that. Simultaneously, to prevent the analysis of potentially un-natural behavior the first 48 hours of detections are removed. It also assigns a unique whale shark ID to each transmitter ID.

```{r select whale shark detections}
# Read file with Whale Shark Tag lists
WS.TAGS$DATE <- as.POSIXct (WS.TAGS$DATE, format="%d/%m/%Y", tz = "Africa/Dar_es_Salaam")
WS.TAGS$NAME <- WS.TAGS$COMMENT <- WS.TAGS$SHARK <- NULL

# Select only whale shark detections 
DET.WS <- MAFIA.DETECTIONS[!is.na (match (MAFIA.DETECTIONS$TRANSMITTERID, WS.TAGS$TRANSMITTERID)), ]

# Remove detections before 48 hours after tagging date
for (i in 1: nrow(WS.TAGS)){
  next2.days <- WS.TAGS$DATE + 60 * 60 * 24 * 2  # Add two days
  replace.index <- as.character (DET.WS$TRANSMITTERID) == as.character (WS.TAGS$TRANSMITTERID[i])
  # Delete rows that are in the tagging day
  DET.WS <- subset (DET.WS, ! (replace.index & (DET.WS$DATETIME < next2.days[i]))) 
}

# Remove unused tags from the factor list
DET.WS$TRANSMITTERID <- factor (DET.WS$TRANSMITTERID)
rm (i, next2.days, replace.index)
```

# Residency models

The follwing function calculates both a time and a lag dependent response variable for the precense of whale sharks. 

```{r function, cache=FALSE}

# Function to calculate the vectors of presence absence
pres.abs.lag <- function (start.date, end.date, sightings, dates){
  # Create a data frame with the detections
  sight <- data.frame (id = sightings, date = dates) %>%
    filter (date >= start.date, date <= end.date) %>%
    mutate (id = factor (id)) %>% 
    arrange (date)
  
  # For each shark we'll start with the first detection only
  individuals <- levels (sight$id)
  # Cycle trough each shark
  presence.absence <- foreach (i=1:length (individuals),
                               .combine = rbind) %dopar% {
    # Find dates in which the shark was present
    dates.present <- sight$date[sight$id == individuals[i]] %>%
      as.numeric ()
    # Establish all dates in which it was tagged (only dates in which there was monitoring)
    dates.tagged <- unique(sight$date)[unique(sight$date) > 
                                         sight$date[match (individuals[i], sight$id)]]
    # Find all possible combinations of dates in which it was tagged
    dates.comb <- as.data.frame (t (combn (dates.tagged, 2))) %>%
      tbl_df()
    names (dates.comb) <- c ("date.1", "date.2")
    dates.comb <- mutate (dates.comb, lag = date.2 - date.1, # Find the lag between given dates
                          # Establish if it was present for in that lag
                          present = (date.1 %in% dates.present) &
                            (date.2 %in% dates.present), 
                          date = as.Date(date.1, origin = "1970-01-01"), 
                          id = individuals[i]) %>%
      select (-date.1, -date.2)
    return (dates.comb)
  } %>%
    mutate (day = yday (date), week = week (date), month = month(date))
  return (presence.absence)
}

```

The model was previously run in a daily basis, however the size of the resulting data frame was too large to be computationally manegable at the scales we are able to work. It will now be redone using weekly bins.

Note that Emmental TZ-003 was tagged twice, but lost it's first tag leaving no detections

```{r}
det.ws <- tbl_df (DET.WS)  # Convert to tbl data frame
ws.tags <- tbl_df (WS.TAGS)
names (det.ws) <- names (det.ws) %>% tolower ()  # change names to lower case 
names (ws.tags) <- names (ws.tags) %>% tolower ()
names (ws.tags)[2] <- "date.tag"

# Using only sharks present in the list of tagged sharks merge data frames
det.ws <- inner_join (det.ws, select (ws.tags, -(size), -(number)))

# Clump in a weekly basis
det.ws <- mutate (det.ws, date.week = cut (datetime, 'week') %>% as.Date ())
aco.week <- ddply (det.ws, "date.week", function (det){
  sharks <- !duplicated (det$ecocean)
  per.week <- data.frame (ecocean = det$ecocean[sharks], 
                          sex = det$sex[sharks],
                          batch = det$sex[sharks], 
                          date.tag = det$date.tag[sharks])
})
```

Now we calculate the response variable:

```{r}
PADet.comb <- pres.abs.lag (start.date = min (aco.week$date.week), 
                            end.date = max (aco.week$date.week),
                            sightings = aco.week$ecocean, 
                            dates = aco.week$date.week)
names (PADet.comb)[4] <- "ecocean"
```

We want to include in the model sex, size and the number of stations to control for effort.

```{r, cache=FALSE}
calc.nStations <- function (ARRAY.EVENTS, PADet){
  # Calculate number of receivers working
  ARRAY.EVENTS <- arrange (ARRAY.EVENTS, DATETIME)
  times.in <- ddply (ARRAY.EVENTS, "STATIONNAME", function (x){
    deployed <- filter (x, EVENT == "DEP", lead (EVENT) == "RET")
    retrieved <- filter (x, EVENT == "RET", lag (EVENT) == "DEP")
    if (nrow (deployed) > 0) {
      y <- data.frame (station = first (x$STATIONNAME), 
                       date.in = deployed$DATETIME, 
                       date.out = retrieved$DATETIME,
                       rec.in = deployed$RECEIVERID,
                       rec.out = retrieved$RECEIVERID)
      return (y)
    } else return (NULL)
  })
  # Calculate array configuration and number of receivers working
  PADet <- ddply (PADet, "date", function (x, times.in){
    stations.listening <- filter (times.in, x$date[1] >= as.Date (date.in), x$date[1] <= as.Date (date.out)) %>% 
      select (station) %>%
      unique()
    x$configuration <- do.call (paste, as.list(stations.listening$station))
    x$nStations <- nrow (stations.listening)
    return (x)
  }, times.in = times.in)
}
  
```


```{r}
# Merge with tagged shark information
PADet.comb <- left_join (PADet.comb, select(ws.tags, ecocean, sex, size, batch))

# Generate a times in out data frame for each receiver
  ARRAY.EVENTS <- arrange (ARRAY.EVENTS, DATETIME)
  times.in <- ddply (ARRAY.EVENTS, "STATIONNAME", function (x){
    deployed <- filter (x, EVENT == "DEP", lead (EVENT) == "RET")
    retrieved <- filter (x, EVENT == "RET", lag (EVENT) == "DEP")
    if (nrow (deployed) > 0) {
      y <- data.frame (station = first (x$STATIONNAME), 
                       date.in = deployed$DATETIME, 
                       date.out = retrieved$DATETIME,
                       rec.in = deployed$RECEIVERID,
                       rec.out = retrieved$RECEIVERID)
      return (y)
    } else return (NULL)
  })

pres <- ddply (PADet.comb, "date", function (x, times.in){
  # Generate weekly approximations
  times.in <- mutate (times.in, w.date.in = cut (date.in, 'week'),
                      w.date.out = cut (date.out, 'week'))
    stations.listening <- filter (times.in, x$date[1] >= as.Date (w.date.in), x$date[1] <= as.Date (w.date.out)) %>% 
      select (station) %>%
      unique()
  y <- data.frame (configuration = do.call (paste, as.list(stations.listening$station)), 
                   nStations = nrow (stations.listening))
    return (y)
  }, times.in = times.in)

PADet.comb <- inner_join (PADet.comb, pres)
```

Now we remove the data of sharks that were known to lost their tags

```{r}
# Delete data for sharks that were known to loose their tag
PADet.comb <- ddply (PADet.comb, "ecocean", function (x, TAG.LOST){
  for (i in 1:length (levels (TAG.LOST$id))){
    if (as.character (TAG.LOST$id[i]) == as.character (first (x$ecocean))){
      out <- filter (x, date < TAG.LOST$date[i])
      } else {out <- x}
    }
  return (out)
  }, TAG.LOST = TAG.LOST) %>% tbl_df()

PADet.comb <- mutate (PADet.comb, lagl = log (lag + 1), 
                      date.id = paste (date, ecocean))
```

Now we try the models

```{r}
mdA <- vector ("list", 0)

mdA[[1]] <- expression(md01 <- gamm (formula = formula (present ~ s (week, bs = "cc") + s (lag, bs = "cr") + sex + size + nStations), data = PADet.comb, family = "binomial", gamma = 1.4))

eval(mdA[[1]])
# 
# # Save the residuals
# rd01 <- residuals (md01$gam, type = "pearson")
# # Plot residuals against variables
# ggplot (PADet.comb) + geom_boxplot (aes (y = rd01, x = ecocean))
# # Evidence of heteogeneous variance per shark
# ggplot (PADet.comb) + geom_boxplot (aes (y = rd01, x = batch))
# # Evidence of heterogeneous variance per batch
# ggplot (PADet.comb) + geom_boxplot (aes (y = rd01, x = as.factor(date))) + coord_flip()
# # Evidence of heterogeneous variance per start date
# ggplot (PADet.comb) + geom_boxplot (aes (y = rd01, x = as.factor(date))) + coord_flip()
# PARes <- PADet.comb ; PARes$res <- md01
```

Secondly we try a model with different smoothers per cohort

```{r}
# Create dummy variables per cohort
PADet.comb <- mutate (PADet.comb, B1 = 0, B2 = 0, B3= 0)
PADet.comb$B1[PADet.comb$batch == "2012-1"] <- 1
PADet.comb$B2[PADet.comb$batch == "2012-2"] <- 1
PADet.comb$B3[PADet.comb$batch == "2014-1"] <- 1

```

```{r, eval = FALSE}
# Only yearly pattern (ITS SINGULAR)
# mdA[[2]] <- expression (md02 <- gamm (formula = formula (present ~ s (week, bs = "cc") + s (week, bs ="cc", by = B1) + s (week, bs ="cc", by = B2) + s (week, bs ="cc", by = B3) + s (lag, bs = "cr") + sex + size + nStations), data = PADet.comb, family = "binomial", gamma = 1.4))

# # Only lag (IT'S SINGULAR)
# mdA[[3]] <- expression (md03 <- gamm (formula = formula (present ~ s (week, bs = "cc") + s (lag, bs = "cr") + s (lag, bs = "cr", by = B1) + s (lag, bs = "cr", by = B2) + s (lag, bs = "cr", by = B3) + sex + size + nStations), data = PADet.comb, family = "binomial", gamma = 1.4)) 
# # Both (IT'S SINGULAR AS WELL)
# mdA[[4]] <- expression (md04 <- gamm (formula = formula (present ~ s (week, bs = "cc") + s (week, bs ="cc", by = B1) + s (week, bs ="cc", by = B2) + s (week, bs ="cc", by = B3) + s (lag, bs = "cr") + s (lag, bs = "cr", by = B1) + s (lag, bs = "cr", by = B2) + s (lag, bs = "cr", by = B3) + sex + size + nStations), data = PADet.comb, family = "binomial", gamma = 1.4)) 

# foreach (i=4:4) %dopar% eval (mdA[[i]])

```

Add random components

```{r}

PADet.comb <- mutate (PADet.comb, date.random = (as.numeric (date) - as.numeric (min (date))) / 7) 
PADet.comb$date.random <- as.factor (PADet.comb$date.random)
PADet.comb$date.id <- as.factor (PADet.comb$date.id)
```

```{r, eval = FALSE}
# Only shark 
mdA[[2]] <- expression(md02 <- gamm (formula = formula (present ~ s (week, bs = "cc") + s (lag, bs = "cr") + sex + size + nStations), data = PADet.comb, family = "binomial", gamma = 1.4, random=list(ecocean=~1)))
# Only date into shark
mdA[[3]] <- expression(md03 <- gamm (formula = formula (present ~ s (week, bs = "cc") + s (lag, bs = "cr") + sex + size + nStations), data = PADet.comb, family = "binomial", gamma = 1.4, random=list(ecocean=~1, date.random = ~1)))
# Shark and date combine
mdA[[4]] <- expression(md04 <- gamm (formula = formula (present ~ s (week, bs = "cc") + s (lag, bs = "cr") + sex + size + nStations), data = PADet.comb, family = "binomial", gamma = 1.4, random=list(date.id=~1)))

for (i in 2:4) eval (mdA[[i]])
```

The matrix is neggative for the combination of sharks and date. We will discard that model. Now we will explore the autocorrelation component for the two models with random components that behaved well. We'll try both only autocorrelation and mooving average 

```{r, eval = FALSE}
PADet.comb <- mutate (PADet.comb, lag.corr <- lag / 7)
# AR1 - Only shark 
mdA[[5]] <- expression(md05 <- gamm (formula = formula (present ~ s (week, bs = "cc") + s (lag, bs = "cr") + sex + size + nStations), data = PADet.comb, family = "binomial", gamma = 1.4, random=list(ecocean=~1), correlation =  corAR1 (form = ~ lag.corr | date.random)))
# AR1 - Only date into shark
mdA[[6]] <- expression(md06 <- gamm (formula = formula (present ~ s (week, bs = "cc") + s (lag, bs = "cr") + sex + size + nStations), data = PADet.comb, family = "binomial", gamma = 1.4, random=list(ecocean=~1, date.random = ~1), correlation =  corAR1 (form = ~ lag.corr | date.random)))
# ARMA11 - Only shark 
mdA[[7]] <- expression(md07 <- gamm (formula = formula (present ~ s (week, bs = "cc") + s (lag, bs = "cr") + sex + size + nStations), data = PADet.comb, family = "binomial", gamma = 1.4, random=list(ecocean=~1), correlation =  corARMA (form = ~ lag.corr | date.random, p = 1, q = 1)))
# ARMA11 - Only date into shark
mdA[[8]] <- expression(md08 <- gamm (formula = formula (present ~ s (week, bs = "cc") + s (lag, bs = "cr") + sex + size + nStations), data = PADet.comb, family = "binomial", gamma = 1.4, random=list(ecocean=~1, date.random = ~1), correlation =  corARMA (form = ~ lag.corr | date.random, p = 1, q = 1)))

for (i in 6:8) eval (mdA[[i]])

```

I noticed there might be a problem with the date, I'll try now modifying the function to include both dates

```{r, eval = FALSE}

# Function to calculate the vectors of presence absence
pres.abs.lag.2 <- function (start.date, end.date, sightings, dates){
  # Create a data frame with the detections
  sight <- data.frame (id = sightings, date = dates) %>%
    filter (date >= start.date, date <= end.date) %>%
    mutate (id = factor (id)) %>% 
    arrange (date)
  
  # For each shark we'll start with the first detection only
  individuals <- levels (sight$id)
  # Cycle trough each shark
  presence.absence <- foreach (i=1:length (individuals),
                               .combine = rbind) %dopar% {
    # Find dates in which the shark was present
    dates.present <- sight$date[sight$id == individuals[i]] %>%
      as.numeric ()
    # Establish all dates in which it was tagged (only dates in which there was monitoring)
    dates.tagged <- unique(sight$date)[unique(sight$date) > 
                                         sight$date[match (individuals[i], sight$id)]]
    # Find all possible combinations of dates in which it was tagged
    dates.comb <- as.data.frame (t (combn (dates.tagged, 2))) %>%
      tbl_df()
    names (dates.comb) <- c ("date.1", "date.2")
    dates.comb <- mutate (dates.comb, lag = date.2 - date.1, # Find the lag between given dates
                          # Establish if it was present for in that lag
                          present = (date.1 %in% dates.present) &
                            (date.2 %in% dates.present), 
                          date.1 = as.Date(date.1, origin = "1970-01-01"), 
                          date.2 = as.Date(date.2, origin = "1970-01-01"), 
                          id = individuals[i])
    return (dates.comb)
  }
  return (presence.absence)
}
```

Now we calculate the response variable:

```{r, eval = FALSE}
PADet.comb.2 <- pres.abs.lag.2 (start.date = min (aco.week$date.week), 
                            end.date = max (aco.week$date.week),
                            sightings = aco.week$ecocean, 
                            dates = aco.week$date.week)
names (PADet.comb)[5] <- "ecocean"
PADet.comb.2 <- mutate (PADet.comb.2, week.1 = week (date.1), week.2 = week (date.2), week.av =( week.2 + week.1)/2)

                    

ma1 <- gamm (present ~ s (week.1, bs = "cc") + s (week.2, bs = "cc") + s (lag, bs = "cr"), family = "binomial", data = PADet.comb.2)

ma2 <- gamm (present ~  s (week.2, bs = "cc") + s (lag, bs = "cr"), family = "binomial", data = PADet.comb.2[sample (nrow(PADet.comb.2), 5000), ])

ma3 <- gamm (present ~  s (week.av, bs = "cc") + s (lag, bs = "cr"), family = "binomial", data = PADet.comb.2[sample (nrow(PADet.comb.2), 5000), ])

date.start <- as.Date (c ("2012-10-10", "2012-12-24", "2014-01-01"))
date.end <- as.Date ("2014-05-01")
pred.df <- foreach (i=1:3, .combine = rbind) %do%{
  pred.df <- expand.grid (date.1 = seq (date.start[i], date.end, by ="week")) %>%
  mutate (date.2 = date.start[1], lag = as.numeric (date.1) - as.numeric (date.start[i]), week.1 = week (date.1), week.2 = week (date.2), )
pred.df$pred <-  predict (ma1$gam, pred.df, type = "response")
pred.df$iii <- i
return (pred.df)
}

ggplot (pred.df,aes (x = date.1, y = pred)) + geom_line(aes (colour = as.factor (iii)))
```

It all looks like bullshit. We'll probably work separatelly on the year basis and the lag basis.


